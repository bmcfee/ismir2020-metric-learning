{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ismir2020-tutorial3-part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL5PuMhss5xJ"
      },
      "source": [
        "# Metric learning for MIR coding demo (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAOEQytVEep8"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBngCwGgPRDa"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to **Editâ†’Notebook** Settings\n",
        "- select **GPU** from the **Hardware Accelerator** drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:\n",
        "\n",
        "> Source: https://colab.research.google.com/notebooks/gpu.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jElR0XOL-c5",
        "outputId": "384498cf-2e1b-416f-98a7-59ba7df67f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(f'TensorFlow version: {tf.__version__}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "TensorFlow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjGgeFJDQK_G"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgiA124AH4qF",
        "outputId": "7df1e015-10d2-436a-bd33-e0729d02aa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Install a Google Drive downloading tool\n",
        "!pip install gdown\n",
        "\n",
        "# Download the dataset\n",
        "!gdown --id 1MycZ6p3Y4OPtQVQXddqbOOTi7f7Wh_8f\n",
        "!gdown --id 17Yl_K84dbADoHude6v_ON6pGqsPCMPPA\n",
        "\n",
        "# Extract mel-spectrograms\n",
        "!tar zxf dim-sim_mel.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MycZ6p3Y4OPtQVQXddqbOOTi7f7Wh_8f\n",
            "To: /content/dim-sim_mel.tar.gz\n",
            "721MB [00:06, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17Yl_K84dbADoHude6v_ON6pGqsPCMPPA\n",
            "To: /content/dim-sim_all.json\n",
            "3.07MB [00:00, 146MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAWWuc9Sgxx"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOGwsOwhTi-F"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Dense, dot, \n",
        "                                     Activation, Input, Flatten, Lambda, Embedding, Concatenate, Layer, Reshape)\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqhE16mUCKt"
      },
      "source": [
        "## Loading the metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_5NB4WzUc7A",
        "outputId": "3cd2ed75-482b-498e-d179-1cd36e4c9751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load json metadata\n",
        "def load_json(file_name):\n",
        "\t\"\"\"Load json.\"\"\"\n",
        "\twith open(file_name, 'r') as f:\n",
        "\t\tdata = json.load(f)\n",
        "\treturn data\n",
        "\t\n",
        "trainset = load_json('dim-sim_all.json')\n",
        "\n",
        "print(f'The number of training examples: {len(trainset)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of training examples: 3781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGy3xAnrVLg_"
      },
      "source": [
        "## Creating data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmpBUf7VKv9"
      },
      "source": [
        "# Setup the batch size and compute steps\n",
        "batch_size = 10\n",
        "steps_per_epoch = int(len(trainset) / batch_size)\n",
        "\n",
        "def data_loader(dataset):\n",
        "\t\"\"\"Data loader.\"\"\"\n",
        "\n",
        "\t# IDs for dataset.\n",
        "\ttriplet_ids = list(dataset.keys())\n",
        "\n",
        "\t# Generator.\n",
        "\tcount_triplet = 0\n",
        "\twhile True:\n",
        "\t\tfor batch_iter in range(0, steps_per_epoch * batch_size, batch_size):\n",
        "\t\t\tif count_triplet > len(dataset) - batch_size:\n",
        "\t\t\t\tcount_triplet = 0 \n",
        "\n",
        "\t\t\tbatch_x, batch_y = batch_triplet_loader(dataset, triplet_ids[count_triplet: count_triplet + batch_size])\n",
        "\t\t\t\n",
        "\t\t\tcount_triplet += batch_size\n",
        "\t\t\tyield batch_x, batch_y\n",
        "\n",
        "def mel_normalization(mel):\n",
        "\t\"\"\"Normalization mel value.\"\"\"\n",
        "\tmel -= 0.20\n",
        "\tmel /= 0.25\n",
        "\treturn mel\n",
        "\n",
        "def batch_triplet_loader(dataset, triplet_ids):\n",
        "\t\"\"\"Batch loader.\"\"\"\n",
        "\n",
        "\tanchor_col = []\n",
        "\tpositive_col = []\n",
        "\tnegative_col = []\n",
        "\tfor triplet_id in triplet_ids:\n",
        "\t\ttriplet = dataset[triplet_id]\n",
        "\t\tanchor_mel = np.load('./dim-sim_mel/' + triplet['anchor']['id'] + '.npy')\n",
        "\t\tpositive_mel = np.load('./dim-sim_mel/' + triplet['positive']['id'] + '.npy')\n",
        "\t\tnegative_mel = np.load('./dim-sim_mel/' + triplet['negative']['id'] + '.npy')\n",
        "\n",
        "\t\t# Normalize mel.\n",
        "\t\tanchor_mel = mel_normalization(anchor_mel)\n",
        "\t\tpositive_mel = mel_normalization(positive_mel)\n",
        "\t\tnegative_mel = mel_normalization(negative_mel)\n",
        "\n",
        "\t\t# Stack batch data.\n",
        "\t\tanchor_col.append(anchor_mel)\n",
        "\t\tpositive_col.append(positive_mel)\n",
        "\t\tnegative_col.append(negative_mel)\n",
        "\n",
        "\t# To array.\n",
        "\tanchor_col = np.array(anchor_col)\n",
        "\tpositive_col = np.array(positive_col)\n",
        "\tnegative_col = np.array(negative_col)\n",
        "\n",
        "\tbatch_x = {\n",
        "\t\t'anchor_input': anchor_col,\n",
        "\t\t'positive_input': positive_col,\n",
        "\t\t'negative_input': negative_col\n",
        "\t}\n",
        "\n",
        "\tbatch_y = np.zeros((batch_size, 2))\n",
        "\tbatch_y[:, 0] = 1\n",
        "\treturn batch_x, batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPl00RrkwpQS"
      },
      "source": [
        "## Creating a backbone model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a652C2R9r2Jl",
        "outputId": "c0bf3646-5541-4299-fe6e-dd1db3d97315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# Basic block.\n",
        "def basic_block(x, num_features, fp_length):\n",
        "\tx = Conv1D(num_features, fp_length, padding='same', use_bias=True, kernel_initializer='he_uniform')(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = Activation('relu')(x)\n",
        "\tx = MaxPool1D(pool_size=fp_length, padding='valid')(x)\n",
        "\treturn x\n",
        "\n",
        "# Backbone model.\n",
        "num_frames = 130\n",
        "x_in = Input(shape = (num_frames, 128))\n",
        "x = basic_block(x_in, 64, 4)\n",
        "x = basic_block(x, 64, 4)\n",
        "x = basic_block(x, 64, 4)\n",
        "x = basic_block(x, 64, 2)\n",
        "x = GlobalAvgPool1D()(x)\n",
        "backbone_model = Model(inputs=[x_in], outputs=[x], name='backbone')\n",
        "backbone_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"backbone\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 130, 128)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 130, 64)           32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 130, 64)           256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 130, 64)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 32, 64)            16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 64)            256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 8, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 8, 64)             16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 64)             256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8, 64)             0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 2, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 2, 64)             8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 64)             256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 64)             0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 64)                0         \n",
            "=================================================================\n",
            "Total params: 75,008\n",
            "Trainable params: 74,496\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfe_nB87wrrj"
      },
      "source": [
        "## Creating a triplet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIVTdV-UsFYS",
        "outputId": "3de14751-615f-4860-edc7-657668ec692e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Triplet model.\n",
        "anchor = Input(shape = (num_frames, 128), name='anchor_input')\n",
        "positive = Input(shape = (num_frames, 128), name='positive_input')\n",
        "negative = Input(shape = (num_frames, 128), name='negative_input')\n",
        "\n",
        "anchor_embedding = backbone_model(anchor)\n",
        "positive_embedding = backbone_model(positive)\n",
        "negative_embedding = backbone_model(negative)\n",
        "\n",
        "# Cosine similarity.\n",
        "dist_fn = Lambda(lambda x: dot(x, axes=1, normalize=True))\n",
        "dist_anchor_positive = dist_fn([anchor_embedding, positive_embedding])\n",
        "dist_anchor_negative = dist_fn([anchor_embedding, negative_embedding])\n",
        "\n",
        "# Stack the similarity scores [1,0] and triplet model.\n",
        "similarity_scores = Lambda(lambda vects: K.stack(vects, axis=1))([dist_anchor_positive, dist_anchor_negative])\n",
        "tripletmodel = Model(inputs=[anchor, positive, negative], outputs=similarity_scores, name='triplet')\n",
        "tripletmodel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"triplet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "anchor_input (InputLayer)       [(None, 130, 128)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positive_input (InputLayer)     [(None, 130, 128)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "negative_input (InputLayer)     [(None, 130, 128)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "backbone (Functional)           (None, 64)           75008       anchor_input[0][0]               \n",
            "                                                                 positive_input[0][0]             \n",
            "                                                                 negative_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1)            0           backbone[0][0]                   \n",
            "                                                                 backbone[1][0]                   \n",
            "                                                                 backbone[0][0]                   \n",
            "                                                                 backbone[2][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 2, 1)         0           lambda[0][0]                     \n",
            "                                                                 lambda[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 75,008\n",
            "Trainable params: 74,496\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kR1Rtjhwuag"
      },
      "source": [
        "## Defining the triplet loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1t_iLs1sHNB"
      },
      "source": [
        "# Define the loss function\n",
        "def triplet_hinge_loss(y_true, y_pred):\n",
        "\t\"\"\"Triplet hinge loss.\"\"\"\n",
        "\t# Always the first dimension of the similarity score is true.\n",
        "\t# Margin is set to 0.1\n",
        "\ty_pos = y_pred[:, 0]\n",
        "\ty_neg = y_pred[:, 1]\n",
        "\tloss = K.mean(K.maximum(0., 0.1 + y_neg - y_pos))\n",
        "\treturn loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j684QQijw3A_"
      },
      "source": [
        "## Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQkvbJ0jLlCx",
        "outputId": "3f4f23d5-6c9c-46e0-ee99-f1c7b733a9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "# Create an optimizer\n",
        "optimizer = Adam(lr=0.001)\n",
        "\n",
        "# Compile the model with the loss\n",
        "tripletmodel.compile(optimizer, loss=triplet_hinge_loss)\n",
        "\n",
        "# Kick off the training!\n",
        "tripletmodel.fit(data_loader(trainset),\n",
        "\t\tepochs=20,\n",
        "\t\tverbose=1,\n",
        "\t\tsteps_per_epoch=steps_per_epoch,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0178\n",
            "Epoch 2/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0146\n",
            "Epoch 3/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0141\n",
            "Epoch 4/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0111\n",
            "Epoch 5/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0100\n",
            "Epoch 6/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0090\n",
            "Epoch 7/20\n",
            "378/378 [==============================] - 8s 22ms/step - loss: 0.0077\n",
            "Epoch 8/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0073\n",
            "Epoch 9/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0060\n",
            "Epoch 10/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0056\n",
            "Epoch 11/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0065\n",
            "Epoch 12/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0060\n",
            "Epoch 13/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0048\n",
            "Epoch 14/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0063\n",
            "Epoch 15/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0051\n",
            "Epoch 16/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0050\n",
            "Epoch 17/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0035\n",
            "Epoch 18/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0043\n",
            "Epoch 19/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0046\n",
            "Epoch 20/20\n",
            "378/378 [==============================] - 8s 21ms/step - loss: 0.0047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78f3b92048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0xqtuWtDbsg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pC0v7qEhM9"
      },
      "source": [
        "## Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q46FYlwUEku6"
      },
      "source": [
        "# Collect unique tracks.\n",
        "track_ids = []\n",
        "triplet_ids = list(trainset.keys())\n",
        "for triplet_id in triplet_ids:\n",
        "\ttriplet = trainset[triplet_id]\n",
        "\tanchor = triplet['anchor']['id']\n",
        "\tpositive = triplet['positive']['id']\n",
        "\tnegative = triplet['negative']['id']\n",
        "\ttrack_ids.append(anchor)\n",
        "\ttrack_ids.append(positive)\n",
        "\ttrack_ids.append(negative)\n",
        "\n",
        "# Load mel.\n",
        "track_id_to_mel = {}\n",
        "for track_id in track_ids:\n",
        "\tmel = np.load('./dim-sim_mel/' + track_id + '.npy')\n",
        "\t# Normalize mel.\n",
        "\tmel = mel_normalization(mel)\n",
        "\tmel = np.expand_dims(mel, axis=0)\n",
        "\ttrack_id_to_mel[track_id] = mel\n",
        "\n",
        "# Prepare input mel-spectrograms\n",
        "mels = np.squeeze(np.array(list(track_id_to_mel.values())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxH4wTCvEsHy"
      },
      "source": [
        "## Extracting embedding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfLihyOrEqnv"
      },
      "source": [
        "# Extract embedding features of the tracks\n",
        "embedding_features = backbone_model.predict(mels, batch_size=64)\n",
        "\n",
        "# Collect the embedding features\n",
        "track_id_to_features = {}\n",
        "for i, track_id in enumerate(track_ids):\n",
        "  track_id_to_features[track_id] = embedding_features[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGz-k7_mFmn5"
      },
      "source": [
        "## Computing distances and scores (triplet prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9X4n-y4BDmY",
        "outputId": "602bd448-14b8-4063-b703-1df582edcb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Define a distance function\n",
        "def euclidean_distance(x1, x2):\n",
        "\treturn np.sqrt(np.maximum(np.sum(np.square(x1 - x2)), 1e-07))\n",
        "\n",
        "# Define an evaluation metric\n",
        "def calculate_accuracy(prediction, groundtruth):\n",
        "\ty_true = np.argmax(groundtruth, axis=-1)\n",
        "\ty_pred = np.argmin(prediction, axis=-1)\n",
        "\taccuracy = float(sum(y_true == y_pred))/len(groundtruth)\n",
        "\treturn accuracy\n",
        "\n",
        "# A placeholder array for triplet prediction \n",
        "prediction = np.zeros((len(triplet_ids), 2))\n",
        "# A placeholder array for the baseline\n",
        "mel_prediction = np.zeros((len(triplet_ids), 2))\n",
        "# Create a groundtruth array\n",
        "groundtruth = np.zeros_like(prediction)\n",
        "groundtruth[:, 0] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9000264480296218\n",
            "0.608833641893679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIIxP1hlDKsl",
        "outputId": "1980e3c0-b452-4a43-80ff-0280a555d25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute distances and scores\n",
        "for i in range(len(triplet_ids)):\n",
        "\ttriplet = trainset[triplet_ids[i]]\n",
        "\tanchor = triplet['anchor']['id']\n",
        "\tpositive = triplet['positive']['id']\n",
        "\tnegative = triplet['negative']['id']\n",
        "\t\n",
        "\tprediction[i, 0] = euclidean_distance(\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_features[anchor].reshape(1, -1), 'l2')),\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_features[positive].reshape(1, -1), 'l2'))\n",
        "\t\t\t)\n",
        "\tprediction[i, 1] = euclidean_distance(\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_features[anchor].reshape(1, -1), 'l2')),\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_features[negative].reshape(1, -1), 'l2'))\n",
        "\t\t\t)\n",
        "\t\n",
        "\t# mel similarity\n",
        "\tmel_prediction[i, 0] = euclidean_distance(\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_mel[anchor].flatten().reshape(1, -1), 'l2')),\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_mel[positive].flatten().reshape(1, -1), 'l2'))\n",
        "\t\t\t)\n",
        "\tmel_prediction[i, 1] = euclidean_distance(\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_mel[anchor].flatten().reshape(1, -1), 'l2')),\n",
        "\t\t\tnp.squeeze(normalize(track_id_to_mel[negative].flatten().reshape(1, -1), 'l2'))\n",
        "\t\t\t)\n",
        "\n",
        "\n",
        "accuracy = calculate_accuracy(prediction, groundtruth)\n",
        "mel_accuracy = calculate_accuracy(mel_prediction, groundtruth)\n",
        "print(f'Triplet model accuracy: {accuracy:.2f}')\n",
        "print(f'Baseline accuracy     : {mel_accuracy:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Triplet model accuracy: 0.90\n",
            "Baseline accuracy     : 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}